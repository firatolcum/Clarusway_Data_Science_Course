{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bc76f5",
   "metadata": {},
   "source": [
    "##  Parametric and NonParametric Machine Learning Algorithms\n",
    "1. Parametric Machine Learning Algorithms:\n",
    "    - Algorithms that simplify the function to a known form are called parametric machine learning algorithms.\n",
    "    - A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples) is called a parametric model. No matter how much data you throw at a parametric model, it won't change its mind about how many parameters it needs.\n",
    "    - Some more examples of parametric machine learning algorithms include:\n",
    "        - Logistic Regression\n",
    "        - Linear Discriminant Analysis\n",
    "        - Perceptron\n",
    "        - Naive Bayes\n",
    "        - Simple Neural Networks.\n",
    "    - Benefits of Parametric Machine Learning Algorithms:\n",
    "        - Simpler : These methods are easier to understand and interpret results.\n",
    "        - Speed : Parametric models are very fast to learn from data.\n",
    "        - Less Data : They don't require as much training data and can work well even if the fit to the data is not perfect.\n",
    "    - Limitations of Parametric Machine Learning Algorithms:\n",
    "        - Constrained : By choosing a functional form these methods are highly constrained to the specified form.\n",
    "        - Limited Complexity : The methods are more suited to simpler problems.\n",
    "        - Poor Fit : In practice the methods are unlikely to match the underlying mapping funciton.\n",
    "\n",
    "2. NonParametric Machine Learning Algorithms:\n",
    "    - Algorithms that do not make strong assumptions about the form of the mapping function are called nonparametric machine learning algorithms. \n",
    "    - NonParametric methods are good when you have a lot of data and no prior knowledge, and when you don't want to worry too much about choosing just the right features.\n",
    "    - Some more examples of popular nonparametric machine learning algorithms are:\n",
    "        - k-Nearest Neighbors\n",
    "        - Decision Trees like CART and C4.5\n",
    "        - Support Vector Machines\n",
    "    - Benefits of Nonparametric Machine Learning Algorithms:\n",
    "        - Flexibility : Capable of fitting a large number of functional forms.\n",
    "        - Power : No assumptions (or weak assumptions) about the underlying function.\n",
    "        - Performance : Can result in higher performance models for prediction.\n",
    "    - Limitations of NonParametric Machine Learning Algorithms:\n",
    "        - More data : Require a lot more training data to estimate the mapping function.\n",
    "        - Slower : A lot slower to train as they often have far more parameters to train.\n",
    "        - Overfitting : More of a risk to overfit the training data and it is harder to explain why specific predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a5bc4",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors Theory (KNN)\n",
    "1. Easy to implement, supervised learning, and non-parametric algorithm.\n",
    "2. Altough it is used for solving both classification and regression problems, it is mostly used for solving classification problems in the industry.\n",
    "3. When the model predicts a new data point, it finds the closest neighbor or neighbors of this new data in the training set and classifies them according to their class.\n",
    "4. KNN Algorithm Steps:\n",
    "    - First, the k parameter is determined. This parameter is the number of neighbours closest to a given point.\n",
    "    - The distance of the new data to be included in the sample data set is calculated individually according to the existing data.\n",
    "    - The closest k neighbors are considered from the related distances.\n",
    "    - The new data is labeled.\n",
    "5. In a KNN Algorithm, predictions are made based on the similarity to the nearest observations.\n",
    "6. KNN works well with smaller dataset because it is a lazy learner.\n",
    "7. KNN is veriy sensitive to noise in the dataet which adversely affect the performance of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddd04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
