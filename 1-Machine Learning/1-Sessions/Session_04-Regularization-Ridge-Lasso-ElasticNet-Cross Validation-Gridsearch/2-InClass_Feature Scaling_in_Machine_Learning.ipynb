{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5cb5e23",
   "metadata": {},
   "source": [
    "### Feature Scaling in Machine Learning\n",
    "- Feature Scaling is a technique of bringing down the values of all the independent features of our dataset on the same scale. Feature scaling helps to do calculations in algorithms very quickly. It is the important stage of data preprocessing.\n",
    "- If we didn't do feature scaling then the machine learning model gives higher weightage to higher values and lower weightage to lower values. Also, takes a lot of time for training the machine learning model.\n",
    "\n",
    "#### Types of Feature Scaling:\n",
    "1. Normalization:\n",
    "    - Normalization is a scaling technique in which values are rescaled between the range 0 to 1.\n",
    "    - To normalize our data, we need to import MinMaxScalar from Sci-Kit learn library and apply it to our dataset. After applying the MinMaxScalar, the minimum value will be zero and the maximum value will be one.\n",
    "2. Standardization:\n",
    "    - Standardization is another scaling technique in which the mean will be equal to zero and the standart deviaton equal to one.\n",
    "    - To standardize our data, we need to import StandartScalar from Sci-Kit learn library and apply it to our dataset.\n",
    "3. Robust Scalar:\n",
    "    - Robust scaling is one of the best scaling techniques when we have outliers present in our dataset.\n",
    "    - It scales the data accordingly to the interquartile range. (IQR = 75 Quartile - 25 Quartile).\n",
    "    - The interquartile range is the middle range where most of the data points exist.\n",
    "4. Gaussian Transformation:\n",
    "    - When our dataset doesn't follow Gaussian/Normal distribution (Bell Curve) then we used Gaussian transformation.\n",
    "    - Let's see different types of Gaussian Transformation:\n",
    "         1. Logarithmic Transformation:\n",
    "             - dataset['Feature_Log'] = np.log(dataset[‘Feature’])\n",
    "         2. Reciprocal Transformation:\n",
    "             - dataset['Feature_Reci'] = 1 / dataset[‘Feature’]\n",
    "         3. Square Root Transformation:\n",
    "             - dataset['Feature_Sqrrt'] = dataset[‘Feature’]**(1/2)\n",
    "         4. Exponential Transformation:\n",
    "             - dataset['Feature_Expo'] = dataset[‘Feature’]**(1/1.2)\n",
    "         5. Box-Cox Transformation:\n",
    "             - dataset['Feature_BC'] , Parameters = stat.boxcox(dataset['Feature'])\n",
    "      \n",
    "#### Frequantly Asked Questions:\n",
    "1. What is Feature Scaling?\n",
    "    - Feature Scaling is a technique to normalize/standardize the independent features present in the dataset in a fixed range.\n",
    "2. Why Feature Scaling?\n",
    "    - Some machine learning algorithms are sensitive, they work on distance formulas and use gradient descent as an optimizer. Having values on the same scales helps gradient descent to reach global minima smoothly. \n",
    "    - For example, Logistic Regression, Support Vector Machine, K Nearest Neighbours, K-Means...\n",
    "3. When to used Feature Scaling?\n",
    "    - We have to use when the range of independent features varying on different scales.\n",
    "4. What are the advantages of using feature scaling?\n",
    "    - Makes training faster.\n",
    "    - Improves the performance.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
