{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7a8eee",
   "metadata": {},
   "source": [
    "## NLP Session-1 PreClass\n",
    "- NLTK, short for Natural Language ToolKit, is a library written in Python for symbolic and statistical Natural Language Processing.\n",
    "- NLTK contains a module called `tokenize()` which further classifies into two sub-categories:\n",
    "    - **Word Tokenize :** We use the `word_tokenize()` method to split a sentence into tokens or words.\n",
    "    - **Sentence Tokenize :** We use the `sent_tokenize` method to split a document or paragraph into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8605beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49ce869",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1eae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " '2002',\n",
       " ',',\n",
       " 'SpaceX',\n",
       " '’',\n",
       " 's',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi-planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self-sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'Mars',\n",
       " '.',\n",
       " 'In',\n",
       " '2008',\n",
       " ',',\n",
       " 'SpaceX',\n",
       " '’',\n",
       " 's',\n",
       " 'Falcon',\n",
       " '1',\n",
       " 'became',\n",
       " 'the',\n",
       " 'first',\n",
       " 'privately',\n",
       " 'developed',\n",
       " 'liquid-fuel',\n",
       " 'launch',\n",
       " 'vehicle',\n",
       " 'to',\n",
       " 'orbit',\n",
       " 'the',\n",
       " 'Earth',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)\n",
    "# NLTK is considering punctuation as a token. Hence for future tasks, we need to remove the punctuations from the initial list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "892be75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1e710ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528195e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars.',\n",
       " 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5487d45f",
   "metadata": {},
   "source": [
    "## Text Cleaning in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a092f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb0bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\"I am writing some very basic english sentences\", \n",
    "            \"I'm just writing it for the demo PURPOSE to make audience understand the basics .\",\n",
    "            \"The point is to _learn HOW it works_ on #simple # data.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b2b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e142d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download() -- in Jupyter\n",
    "# python -m nltk.downloader all -- or in Command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc334cd",
   "metadata": {},
   "source": [
    "### Step 1 - Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6aa1c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am writing some very basic english sentences',\n",
       " \"i'm just writing it for the demo purpose to make audience understand the basics .\",\n",
       " 'the point is to _learn how it works_ on #simple # data.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "raw_docs = [doc.lower() for doc in raw_docs]\n",
    "raw_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4099e91f",
   "metadata": {},
   "source": [
    "### Step 2 - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461c49f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'],\n",
       " ['i',\n",
       "  \"'m\",\n",
       "  'just',\n",
       "  'writing',\n",
       "  'it',\n",
       "  'for',\n",
       "  'the',\n",
       "  'demo',\n",
       "  'purpose',\n",
       "  'to',\n",
       "  'make',\n",
       "  'audience',\n",
       "  'understand',\n",
       "  'the',\n",
       "  'basics',\n",
       "  '.'],\n",
       " ['the',\n",
       "  'point',\n",
       "  'is',\n",
       "  'to',\n",
       "  '_learn',\n",
       "  'how',\n",
       "  'it',\n",
       "  'works_',\n",
       "  'on',\n",
       "  '#',\n",
       "  'simple',\n",
       "  '#',\n",
       "  'data',\n",
       "  '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838d55db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i am writing some very basic english sentences'],\n",
       " [\"i'm just writing it for the demo purpose to make audience understand the basics .\"],\n",
       " ['the point is to _learn how it works_ on #simple # data.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_token = [sent_tokenize(doc) for doc in raw_docs]\n",
    "sent_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97b5eb",
   "metadata": {},
   "source": [
    "### Step 3 - Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17addf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'am', 'writing', 'some', 'very', 'basic', 'english', 'sentences'], ['i', 'm', 'just', 'writing', 'it', 'for', 'the', 'demo', 'purpose', 'to', 'make', 'audience', 'understand', 'the', 'basics'], ['the', 'point', 'is', 'to', 'learn', 'how', 'it', 'works', 'on', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u\"\", token)\n",
    "        if not new_token == u\"\":\n",
    "            new_review.append(new_token)\n",
    "            \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd631b",
   "metadata": {},
   "source": [
    "### Step 4 - Removing Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17070679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'basic', 'english', 'sentences'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basics'], ['point', 'learn', 'works', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words(\"english\"):\n",
    "            new_term_vector.append(word)\n",
    "            \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "    \n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407726ec",
   "metadata": {},
   "source": [
    "### Step 5 - Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e36926a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['writing', 'basic', 'english', 'sentence'], ['writing', 'demo', 'purpose', 'make', 'audience', 'understand', 'basic'], ['point', 'learn', 'work', 'simple', 'data']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        #final_doc.append(porter.stem(word)) # Stemming\n",
    "        final_doc.append(wordnet.lemmatize(word)) # Lemmatization\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(preprocessed_docs)\n",
    "\n",
    "# Stemming just cut the words from the end.\n",
    "# Lemmatization will refer to a dictionary and convert it to the meaningful root word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97c388",
   "metadata": {},
   "source": [
    "## CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d857e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"hello, my name is Aman and I am  a data scientist.\"]\n",
    "text1 = [\"You are watching unfold data science\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d755290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da3594d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 4,\n",
       " 'my': 6,\n",
       " 'name': 7,\n",
       " 'is': 5,\n",
       " 'aman': 1,\n",
       " 'and': 2,\n",
       " 'am': 0,\n",
       " 'data': 3,\n",
       " 'scientist': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n",
    "# Burada her bir kelimeye bir index numarası atandı.\n",
    "# Aynı zamanda cleaning de yaptığı için bazı kelimeler yok oldu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bbae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newvector = vectorizer.transform(text1)\n",
    "newvector.toarray()\n",
    "# The word \"data\" is present in the other document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32455d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = [\"You are watching unfold data science. Aman Aman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79b4c286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newvector2 = vectorizer.transform(text2)\n",
    "newvector2.toarray()\n",
    "# Burda 1. index'e 2 sayısı atandı çünkü 1. index \"aman\" ı temsil ediyor ve text2 içinde 2 kez geçiyor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b65911",
   "metadata": {},
   "source": [
    "## TF - IDF (Term Frequency - Inverse Document Frequency)\n",
    "- Purpose of TF-IDF is to highlight words which are frequent in a document but not across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d40998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"Aman is a data scientist in India\", \"This is unfold data science\", \"Data Science is a promising career.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d0052cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb49c385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aman': 0,\n",
       " 'is': 5,\n",
       " 'data': 2,\n",
       " 'scientist': 8,\n",
       " 'in': 3,\n",
       " 'india': 4,\n",
       " 'this': 9,\n",
       " 'unfold': 10,\n",
       " 'science': 7,\n",
       " 'promising': 6,\n",
       " 'career': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0198a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69314718, 1.69314718, 1.        , 1.69314718, 1.69314718,\n",
       "       1.        , 1.69314718, 1.28768207, 1.69314718, 1.69314718,\n",
       "       1.69314718])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_\n",
    "# Burada text içindeki her bir cümlede \"data\" kelimesi(2.index) geçtiği için onun katsayısını 1 verdi. Belirleyici değil.Etkisiz\n",
    "# Ayrıca \"is\" ifadesi(5. index) de aynı şekilde her cümlede geçtiği için katsayı olarak 1 aldı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac3e5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aman is a data scientist in India'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_input = text[0]\n",
    "text_as_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f2198e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46138073, 0.        , 0.27249889, 0.46138073, 0.46138073,\n",
       "        0.27249889, 0.        , 0.        , 0.46138073, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = vectorizer.transform([text_as_input])\n",
    "vector.toarray()\n",
    "# \"Aman is a data scientist in India\" cümlesi içinde olmayan kelimeleri belirten index numaraları 0 oldu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beadbc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Science is a promising career.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_input = text[2]\n",
    "text_as_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6ec0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vectorizer.transform([text_as_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f362b2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.55249005, 0.32630952, 0.        , 0.        ,\n",
       "        0.32630952, 0.55249005, 0.42018292, 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda97299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf517c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8db35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d0017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
